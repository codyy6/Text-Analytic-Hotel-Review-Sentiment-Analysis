{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEAR2xqLE7In40UpfbhHMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codyy6/Text-Analytic-Hotel-Review-Sentiment-Analysis/blob/main/Question_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demonstrate** word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output."
      ],
      "metadata": {
        "id": "oKy9pooa9vB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Function**"
      ],
      "metadata": {
        "id": "YOvR1oEW-GYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#read file\n",
        "file_path = '/content/Data_1.txt'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "#nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwYA1xPwGCT0",
        "outputId": "4d653bf8-13ac-4034-eb92-e3424ea10643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular Expression**"
      ],
      "metadata": {
        "id": "6RA6mx42-KK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#read file\n",
        "file_path = '/content/Data_1.txt'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "#Regular expression\n",
        "tokens_regex = list(filter(None,re.split(r'\\s+|[,.:!?]', text)))\n",
        "\n",
        "\n",
        "print(\"Word tokenization using Regular expression:\")\n",
        "print(tokens_regex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VibqjBMbGCpw",
        "outputId": "4c47d35f-8ac7-4cf2-ad43-4759305e4d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization using Regular expression:\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', 'facts', 'and', 'opinions', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', 'events', 'and', 'their', 'properties', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments', 'appraisals', 'or', 'feelings', 'toward', 'entities', 'events', 'and', 'their', 'properties']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK **PACKAGES**"
      ],
      "metadata": {
        "id": "xgAhwqHk-E7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. word tokenize"
      ],
      "metadata": {
        "id": "Kqz7vVeB-R6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Read the file\n",
        "file_path = '/content/Data_1.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokens_word_tokenize = word_tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Word tokenization using word_tokenize:\")\n",
        "print(tokens_word_tokenize)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFgxMAlfOejY",
        "outputId": "1387f9d8-fe02-4df2-844d-7574edec9550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization using word_tokenize():\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. TreebankWordTokenize"
      ],
      "metadata": {
        "id": "qTocabPs-Why"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# Read the file\n",
        "file_path = '/content/Data_1.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokenizer_treebank = TreebankWordTokenizer()\n",
        "\n",
        "# TreebankWordTokenizer\n",
        "tokens_treebank = tokenizer_treebank.tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Word tokenization using TreebankWordTokenizer:\")\n",
        "print(tokens_treebank)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vem-gb2N-SJ",
        "outputId": "aa02d8db-957c-4e58-d466-fe102d9b3b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization using TreebankWordTokenizer():\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Regexp tokenize"
      ],
      "metadata": {
        "id": "fLoD4TAO-aIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Read the file\n",
        "file_path = '/content/Data_1.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokenizer_regexp = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "\n",
        "# RegexpTokenizer\n",
        "tokens_regexp = tokenizer_regexp.tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Word tokenization using RegexpTokenizer:\")\n",
        "print(tokens_regexp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71FRQ4klOxvW",
        "outputId": "38f496bf-bf30-440e-bac2-dd252cfb3371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization using RegexpTokenizer():\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’s', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Wordpunct tokenize"
      ],
      "metadata": {
        "id": "eprBBgG1-d6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# Read the file\n",
        "file_path = '/content/Data_1.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokenizer_wordpunct = WordPunctTokenizer()\n",
        "\n",
        "# WordPunctTokenizer\n",
        "tokens_wordpunct = tokenizer_wordpunct.tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Word tokenization using WordPunctTokenizer:\")\n",
        "print(tokens_wordpunct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iRqrok3POJ5",
        "outputId": "1c2e435f-82fc-4e1f-c30b-fa2e6b9c452d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization using WordPunctTokenizer():\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. sent tokenize"
      ],
      "metadata": {
        "id": "sxdHv74r-gZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# File path\n",
        "file_path = '/content/Data_1.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# sent_tokenize\n",
        "tokens_sent = sent_tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Sentence tokenization using sent_tokenize:\")\n",
        "print(tokens_sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe0gaJv4TI51",
        "outputId": "878eb289-c660-4202-b557-08b18a64f215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokenization using sent_tokenize:\n",
            "['Textual information in the world can be broadly categorized into two main types: facts and opinions.', 'Facts are objective expressions about entities, events, and their properties.', 'Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demonstrate** stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus."
      ],
      "metadata": {
        "id": "ewWQk2OI-nD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# read file\n",
        "file_path = 'Data_1.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    sampleText = file.read()\n",
        "\n",
        "# Tokenize the text into words\n",
        "Tokens = word_tokenize(sampleText)\n",
        "print(\"There are \", len(Tokens), \"words in this text\\n\")\n",
        "\n",
        "# stop words\n",
        "stopTokens = stopwords.words(\"english\") + list(string.punctuation)\n",
        "filteredTokens = []\n",
        "\n",
        "# remove stop words and punctuations\n",
        "for w in Tokens:\n",
        "    if w.lower() not in stopTokens:\n",
        "        filteredTokens.append(w)\n",
        "\n",
        "print(\"There are \", len(filteredTokens), \"words in this text after removing stop words and punctuations\\n\")\n",
        "print(filteredTokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr1G7w0rG2O8",
        "outputId": "f94eaf65-536a-4802-9670-1a868fbfbbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  56 words in this text\n",
            "\n",
            "There are  30 words in this text after removing stop words and punctuation\n",
            "\n",
            "['Textual', 'information', 'world', 'broadly', 'categorized', 'two', 'main', 'types', 'facts', 'opinions', 'Facts', 'objective', 'expressions', 'entities', 'events', 'properties', 'Opinions', 'usually', 'subjective', 'expressions', 'describe', 'people', '’', 'sentiments', 'appraisals', 'feelings', 'toward', 'entities', 'events', 'properties']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FCcWwDdY9t9R"
      }
    }
  ]
}